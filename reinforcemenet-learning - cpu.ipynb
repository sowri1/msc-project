{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"./DroneLib.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function get_state(drone::Drone)\n",
    "#     return [drone.position..., drone.velocity..., drone.orientation..., drone.angular_velocity...]\n",
    "# end\n",
    "\n",
    "# function apply_changes!(drone::Drone, du)\n",
    "#     # This function should update the drone's state based on the changes calculated by drone_dynamics!.\n",
    "#     # For example:\n",
    "#     drone.position .+= du[1:3]\n",
    "#     drone.velocity .+= du[4:6]\n",
    "#     drone.orientation .+= du[7:9]\n",
    "#     drone.angular_velocity .+= du[10:12]\n",
    "# end\n",
    "\n",
    "# function ReinforcementLearningBase.step!(env::DroneEnv, action)\n",
    "#     # Apply the action (change in RPMs or rotor forces), then update drone's state.\n",
    "#     # Use the drone_dynamics! function here.\n",
    "#     du = zeros(12) #assuming the size of du matches with the state size\n",
    "#     drone_dynamics!(du, get_state(env.drone), (t) -> action, 0.0) # assuming that drone_dynamics! is defined in a way that is compatible with this use\n",
    "\n",
    "#     # Update drone state with the changes\n",
    "#     apply_changes!(env.drone, du)\n",
    "\n",
    "#     # Also update the reward and done status.\n",
    "#     env.reward = reward(env, action)\n",
    "#     env.done = is_done(env)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DroneEnv()\n",
    "\n",
    "# total_reward = 0.0\n",
    "# num_steps = 0\n",
    "\n",
    "# trajectory = []\n",
    "\n",
    "# # Start the episode\n",
    "# while !RLBase.is_terminated(env)\n",
    "#     # Select a random action\n",
    "#     action = rand(RLBase.action_space(env))\n",
    "\n",
    "#     # Apply the action to the environment\n",
    "#     reward, done = env(action)\n",
    "# \t# println(\"Reward: \", reward)\n",
    "\n",
    "#     # Update the total reward and number of steps\n",
    "#     total_reward += reward\n",
    "#     num_steps += 1\n",
    "\n",
    "#     # Save the drone's position\n",
    "#     push!(trajectory, env.drone.position)\n",
    "\n",
    "#     if done\n",
    "#         break\n",
    "#     end\n",
    "# end\n",
    "# println(\"Episode finished after \", num_steps, \" steps. Total reward: \", total_reward)\n",
    "# println(\"Final position: \", env.drone.position)\n",
    "\n",
    "# ### 3d Plot the trajectory\n",
    "# x = [trajectory[i][1] for i in 1:length(trajectory)]\n",
    "# y = [trajectory[i][2] for i in 1:length(trajectory)]\n",
    "# z = [trajectory[i][3] for i in 1:length(trajectory)]\n",
    "\n",
    "# # plotlyjs()\n",
    "# plot(x, y, z, label=\"trajectory\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\", title=\"Drone Trajectory\", legend=:bottomright)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Start the reinforcement learning\n",
    "# ### Implement a Deep learning model for Q-learning algorithm to learn the optimal policy\n",
    "# ### The state space is the drone's position and velocity\n",
    "# ### The action space is the change in RPMs of the propellers\n",
    "# ### The reward function is the distance to the target\n",
    "# ### The termination condition is when the drone reaches the target or crashes\n",
    "\n",
    "# ### Define the Q-network\n",
    "# struct QNetwork\n",
    "# \tenv::DroneEnv\n",
    "# end\n",
    "\n",
    "# ### Define the DeepQLearningPolicy\n",
    "# struct DeepQLearningPolicy\n",
    "# \tenv::DroneEnv\n",
    "# \tQ::QNetwork\n",
    "# \tepsilon::Float64\n",
    "# \tgamma::Float64\n",
    "# \talpha::Float64\n",
    "# \tepsilon_decay::Float64\n",
    "# \tgamma_decay::Float64\n",
    "# \talpha_decay::Float64\n",
    "# \tepsilon_min::Float64\n",
    "# \tgamma_min::Float64\n",
    "# \talpha_min::Float64\n",
    "# end\n",
    "\n",
    "# policy = DeepQLearningPolicy(env, QNetwork(env), 0.1, 0.99, 0.1, 0.1, 0.1, 0.1, 0.1, 0.95, 0.1)\n",
    "\n",
    "# ### Define the Q-learning algorithm\n",
    "# function QLearning(policy::DeepQLearningPolicy, num_episodes::Int64)\n",
    "# \t# Initialize the Q-network\n",
    "# \tQ = policy.Q\n",
    "# \t# Initialize the total reward\n",
    "# \ttotal_reward = 0.0\n",
    "# \t# Initialize the number of steps\n",
    "# \tnum_steps = 0\n",
    "# \t# Initialize the trajectory\n",
    "# \ttrajectory = []\n",
    "# \t# Initialize the episode\n",
    "# \tfor i in 1:num_episodes\n",
    "# \t\t# Initialize the environment\n",
    "# \t\tenv = policy.env\n",
    "# \t\t# Initialize the episode\n",
    "# \t\twhile !RLBase.is_terminated(env)\n",
    "# \t\t\t# Select an action\n",
    "# \t\t\taction = rand(RLBase.action_space(env))\n",
    "# \t\t\t# Apply the action to the environment\n",
    "# \t\t\treward, done = env(action)\n",
    "# \t\t\t# Update the total reward and number of steps\n",
    "# \t\t\ttotal_reward += reward\n",
    "# \t\t\tnum_steps += 1\n",
    "# \t\t\t# Save the drone's position\n",
    "# \t\t\tpush!(trajectory, env.drone.position)\n",
    "# \t\t\t# Update the Q-network\n",
    "# \t\t\tQ = update(Q, env, action, reward, done)\n",
    "# \t\t\t# Update the policy\n",
    "# \t\t\tpolicy = update(policy, Q)\n",
    "# \t\t\tif done\n",
    "# \t\t\t\tbreak\n",
    "# \t\t\tend\n",
    "# \t\tend\n",
    "# \tend\n",
    "# \treturn policy, Q, total_reward, num_steps, trajectory\n",
    "# end\n",
    "\n",
    "# ### Define the update function for the Q-network\n",
    "# function update(Q::QNetwork, env::DroneEnv, action, reward, done)\n",
    "# \t# Update the Q network based on the reward and done status\n",
    "\t\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement the PPO algorithm\n",
    "### Define the PPO policy\n",
    "mutable struct PPOPolicy\n",
    "\tenv::DroneEnv\n",
    "\tpolicy::Chain\n",
    "\tepsilon::Float64\n",
    "\tgamma::Float64\n",
    "\talpha::Float64\n",
    "\tepsilon_decay::Float64\n",
    "\tgamma_decay::Float64\n",
    "\talpha_decay::Float64\n",
    "\tepsilon_min::Float64\n",
    "\tgamma_min::Float64\n",
    "\talpha_min::Float64\n",
    "end\n",
    "\n",
    "# function PPOPolicy(env::DroneEnv, policy::Chain, epsilon::Float64, gamma::Float64, alpha::Float64, epsilon_decay::Float64, gamma_decay::Float64, alpha_decay::Float64, epsilon_min::Float64, gamma_min::Float64, alpha_min::Float64)\n",
    "# \treturn PPOPolicy(env, policy, epsilon, gamma, alpha, epsilon_decay, gamma_decay, alpha_decay, epsilon_min, gamma_min, alpha_min)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gaussian_policy(network::Chain, state::Array{Float64,1})\n",
    "\tmu, log_sigma = network(state)  # Assume the network outputs means and log standard deviations\n",
    "    sigma = exp.(log_sigma)\n",
    "    return MvNormal(mu, sigma)  # Return a multivariate normal distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function prob(policy::PPOPolicy, state::Array{Float64,1}, action::Array{Float64,1})\n",
    "\tdistribution = gaussian_policy(policy.policy, state)\n",
    "    return pdf(distribution, action)  # Probability density function for the given action\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_action(policy::PPOPolicy, state::Array{Float64,1})\n",
    "\tdistribution = gaussian_policy(policy.policy, state)\n",
    "\taction = rand(distribution)\n",
    "\treturn action\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the update function for the PPO policy\n",
    "function update(policy::PPOPolicy, current_batch, optimizer)\n",
    "\tfor (state, action, reward, new_state, done) in current_batch\n",
    "\t\tvalue = reward + policy.gamma * (done ? 0.0 : prob(policy, new_state, get_action(policy, state)))\n",
    "\t\tadvantage = value - prob(policy, state, action)\n",
    "\t\tratio = prob(policy, state, action) / prob(policy, state, action)\n",
    "\t\tloss = -min(ratio * advantage, clamp(ratio, 1 - policy.epsilon, 1 + policy.epsilon) * advantage)\n",
    "\t\tgrads = gradient(() -> loss, params(policy.policy))\n",
    "\t\tFlux.Optimise.update!(optimizer, params(policy.policy), grads)\n",
    "\tend\n",
    "end\n",
    "\n",
    "# function update(policy::PPOPolicy, current_batch, optimizer)\n",
    "#     for (state, action, reward, new_state, done) in current_batch\n",
    "#         # value = estimate_value(policy, state)  # Use value function to estimate state value\n",
    "#         # old_prob = prob(policy, state, action)  # Probability under the previous policy\n",
    "\n",
    "#         new_prob = prob(policy, new_state, get_action(policy, new_state))  # Probability under the current policy\n",
    "#         ratio = new_prob / old_prob\n",
    "\n",
    "#         advantage = reward + policy.gamma * (done ? 0.0 : value) - value  # Compute advantage using value function\n",
    "\n",
    "#         surrogate1 = ratio * advantage\n",
    "#         surrogate2 = clamp(ratio, 1 - policy.epsilon, 1 + policy.epsilon) * advantage\n",
    "#         loss = -min(surrogate1, surrogate2)\n",
    "\n",
    "#         Flux.Optimise.update!(optimizer, params(policy.policy)) do\n",
    "#             grads = gradient(() -> loss, params(policy.policy))\n",
    "#             grads = Flux.Tracker.update!(optimizer, grads)\n",
    "#             return grads\n",
    "#         end\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the PPO algorithm\n",
    "function PPO(policy::PPOPolicy, num_episodes::Int64, batch_size::Int64)\n",
    "\t# Initialize the total reward\n",
    "\ttotal_reward = Base.Threads.Atomic{Float64}(0.0)\n",
    "\t# Initialize the number of steps\n",
    "\tnum_steps = Base.Threads.Atomic{Int64}(0.0)\n",
    "\t# Initialize the trajectory\n",
    "\ttrajectories = []\n",
    "\toptimizer = ADAM(0.01)\n",
    "\n",
    "\t# Initialize thread-local buffers\n",
    "\t# local_batches = Vector{Tuple{Array{Float64,1}, Array{Float64,1}, Float64, Array{Float64,1}, Bool}}[]\n",
    "\n",
    "\t# Initialize the episode\n",
    "\t@threads for i in 1:num_episodes\n",
    "\t\t# Initialize the environment\n",
    "\t\tenv = policy.env\n",
    "\t\ttrajectory = []\n",
    "\t\tlocal_batch = []\n",
    "\n",
    "\t\t# Reset the environment\n",
    "\t\tRLBase.reset!(env)\n",
    "\n",
    "\t\t# Initialize the episode\n",
    "\t\twhile !RLBase.is_terminated(env)\n",
    "\t\t\tstate = RLBase.state(env)\n",
    "\t\t\taction = get_action(policy, state)\n",
    "\n",
    "\t\t\t# Apply the action to the environment\n",
    "\t\t\treward, done = step!(env, action)\n",
    "\t\t\t# Update the total reward and number of steps\n",
    "\t\t\tBase.Threads.atomic_add!(total_reward, reward)\n",
    "\t\t\tBase.Threads.atomic_add!(num_steps, 1)\n",
    "\t\t\t# Save the drone's position\n",
    "\t\t\tpush!(trajectory, env.drone.position)\n",
    "\n",
    "\t\t\tpush!(local_batch, (state, action, reward, state, done))\n",
    "\n",
    "\t\t\t# Update the policy if batch size is reached\n",
    "\t\t\tif length(local_batch) >= batch_size\n",
    "\t\t\t\tupdate(policy, local_batch, optimizer)\n",
    "\t\t\t\tlocal_batch = []\n",
    "\t\t\tend\n",
    "\n",
    "\t\t\tif done\n",
    "\t\t\t\tbreak\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\n",
    "\t\tupdate(policy, local_batch, optimizer)\n",
    "\n",
    "\t\tpush!(trajectories, trajectory)\n",
    "\n",
    "\t\t# Decay the exploration\n",
    "\t\tpolicy.epsilon = max(policy.epsilon * policy.epsilon_decay, policy.epsilon_min)\n",
    "\tend\n",
    "\n",
    "\treturn policy, total_reward, num_steps, trajectories\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pos = Array([1.0, 1.0, 1.0])\n",
    "env = DroneEnv(target_pos)\n",
    "### Create the PPO policy# Define the policy network\n",
    "policy_chain = Chain(\n",
    "    Dense(16, 64, relu),\n",
    "    Dense(64, 64, relu),\n",
    "    Dense(64, 8),  # Assume your state is 4-dimensional, and you output mean and log standard deviation for 4-dimensional action\n",
    "    x -> (x[1:4], x[5:8])  # Split the output into two parts,\n",
    "    # the first part is the mean, the second part is the log standard deviation\n",
    ")\n",
    "\n",
    "policy = PPOPolicy(env, policy_chain, 0.1, 0.99, 0.1, 0.1, 0.1, 0.1, 0.1, 0.95, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the PPO algorithm\n",
    "for i in 1:100\n",
    "\tpolicy, total_reward, num_steps, trajectory = PPO(policy, 5000, 16)\n",
    "\tprintln(\"Episode $i: Total reward: $total_reward, Number of steps: $num_steps\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in 1:length(trajectory)\n",
    "\ttraj = trajectory[j]\n",
    "\tx = [traj[i][1] for i in 1:length(traj)]\n",
    "\ty = [traj[i][2] for i in 1:length(traj)]\n",
    "\tz = [traj[i][3] for i in 1:length(traj)]\n",
    "\t### Plot the trajectory\n",
    "\tplot(x, y, z, label=\"trajectory\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\", title=\"Drone Trajectory $j\", legend=:bottomright)\n",
    "\t### Save the plot\n",
    "\tsavefig(\"trajectories/trajectory-$j.png\")\n",
    "# plot!(x, y, z)\n",
    "### Plot the drone\n",
    "# scatter!([x[end]], [y[end]], [z[end]], label=\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run a sample Episode\n",
    "env = DroneEnv([1.0, 1.0, 1.0])\n",
    "\n",
    "total_reward = 0.0\n",
    "num_steps = 0\n",
    "\n",
    "trajectory = []\n",
    "\n",
    "while !RLBase.is_terminated(env)\n",
    "\t# Select a random action\n",
    "\taction = get_action(policy, RLBase.state(env))\n",
    "\n",
    "\t# Apply the action to the environment\n",
    "\treward, done = step!(env, action)\n",
    "\t# println(\"Reward: \", reward)\n",
    "\n",
    "\t# Update the total reward and number of steps\n",
    "\ttotal_reward += reward\n",
    "\tnum_steps += 1\n",
    "\n",
    "\t# Save the drone's position\n",
    "\tpush!(trajectory, env.drone.position)\n",
    "\n",
    "\tif done\n",
    "\t\tbreak\n",
    "\tend\n",
    "end\n",
    "println(\"Episode finished after \", num_steps, \" steps. Total reward: \", total_reward)\n",
    "println(\"Final position: \", env.drone.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PIDController\n",
    "    kp::Float64\n",
    "    ki::Float64\n",
    "    kd::Float64\n",
    "    integral::Array{Float64, 1}\n",
    "    prev_error::Array{Float64, 1}\n",
    "end\n",
    "\n",
    "function PIDController(kp::Float64, ki::Float64, kd::Float64)\n",
    "    integral = zeros(3)\n",
    "    prev_error = zeros(3)\n",
    "    PIDController(kp, ki, kd, integral, prev_error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_pid!(pid::PIDController, error::Array{Float64, 1})\n",
    "    pid.integral += error\n",
    "    derivative = error - pid.prev_error\n",
    "    pid.prev_error = error\n",
    "\n",
    "    pid_output = pid.kp .* error + pid.ki .* pid.integral + pid.kd .* derivative\n",
    "    return pid_output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_random_action() \n",
    "\tbase_rpm = 7900.0\n",
    "\taction = [base_rpm, base_rpm, base_rpm, base_rpm]\n",
    "\t# action = action + rand(4) * 1000\n",
    "\treturn action\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a PID Control for the episode\n",
    "\n",
    "env = DroneEnv([1.0, 1.0, 1.0], 10.0)\n",
    "env.action = [7500.0, 7500.0, 7500.0, 7500.0]\n",
    "total_reward = 0.0\n",
    "num_steps = 0\n",
    "\n",
    "trajectory = []\n",
    "\n",
    "while !RLBase.is_terminated(env)\n",
    "\t# Select a random action\n",
    "\taction = PIDControlDroneEnvAction(env)\n",
    "\t# action = get_random_action()\n",
    "\t# if num_steps > 95\n",
    "\t# \taction = [0.0, 0.0, 0.0, 0.0]\n",
    "\t# end\n",
    "\t# Apply the action to the environment\n",
    "\treward, done = step!(env, action)\n",
    "\t# println(\"Reward: \", reward)\n",
    "\n",
    "\t# Update the total reward and number of steps\n",
    "\ttotal_reward += reward\n",
    "\tnum_steps += 1\n",
    "\n",
    "\t# Save the drone's position\n",
    "\tpush!(trajectory, env.drone.position)\n",
    "\n",
    "\tif done\n",
    "\t\tbreak\n",
    "\tend\n",
    "end\n",
    "\n",
    "println(env.drone.position, env.drone.velocity, env.drone.angular_velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "\n",
    "a += -9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "89f66e31d3dc4ae08656c8051cd62a33",
   "lastKernelId": "379da302-c674-4dad-b83f-33350176c198"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
