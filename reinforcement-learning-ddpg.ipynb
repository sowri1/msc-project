{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_model (generic function with 2 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include(\"./DroneLibV2.jl\")\n",
    "include(\"./ReinforceDDPGDroneLib.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUNoise([0.0, 0.0, 0.0, 0.0], 0.15, 0.0, 0.2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize environment\n",
    "state_dim = 12 + 3 + 4\n",
    "action_dim = 4\n",
    "MAX_ACTION = 14000.0\n",
    "MAX_ACTION = convert(Float64, MAX_ACTION)\n",
    "\n",
    "# Initialize replay buffer\n",
    "buffer = deserialize(\"training-buffer_1.dat\")\n",
    "# buffer = ReplayBuffer(state_dim, action_dim, 10000000)\n",
    "\n",
    "# Initialize noise\n",
    "noise = OUNoise(action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDPGAgent(Actor(Chain(Dense(19 => 256, relu), BatchNorm(256), Dense(256 => 512, relu), BatchNorm(512), Dense(512 => 1024, relu), BatchNorm(1024), Dense(1024 => 1024, relu), BatchNorm(1024), Dense(1024 => 256, relu), BatchNorm(256), Dense(256 => 64, relu), BatchNorm(64), Dense(64 => 4, σ))), Actor(Chain(Dense(19 => 256, relu), BatchNorm(256), Dense(256 => 512, relu), BatchNorm(512), Dense(512 => 1024, relu), BatchNorm(1024), Dense(1024 => 1024, relu), BatchNorm(1024), Dense(1024 => 256, relu), BatchNorm(256), Dense(256 => 64, relu), BatchNorm(64), Dense(64 => 4, σ))), Critic(Chain(Dense(23 => 256), BatchNorm(256), Dense(256 => 1024), BatchNorm(1024), Dense(1024 => 256), BatchNorm(256), Dense(256 => 128), BatchNorm(128), Dense(128 => 1))), Critic(Chain(Dense(23 => 256), BatchNorm(256), Dense(256 => 1024), BatchNorm(1024), Dense(1024 => 256), BatchNorm(256), Dense(256 => 128), BatchNorm(128), Dense(128 => 1))), Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), Adam(0.002, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), 0.99, 0.005, 14000.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize agent\n",
    "# agent = DDPGAgent(state_dim, action_dim, MAX_ACTION)\n",
    "agent = DDPGAgent(state_dim, action_dim, MAX_ACTION, \"agent_critic_model_1.bson\", \"agent_actor_model_1.bson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mPerforming scalar indexing on task Task (runnable) @0x000002c843ff22f0.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mInvocation of getindex resulted in scalar indexing of a GPU array.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis is typically caused by calling an iterating implementation of a method.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mand therefore are only permitted from the REPL for prototyping purposes.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mIf you did intend to index this array, annotate the caller with @allowscalar.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ GPUArraysCore C:\\Users\\sowri\\.julia\\packages\\GPUArraysCore\\uOYfN\\src\\GPUArraysCore.jl:106\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Episodic Reward: -62294.551850336895. Total steps: 517\n",
      "Episode: 2, Episodic Reward: -204056.00335209144. Total steps: 1049\n",
      "Episode: 3, Episodic Reward: -22990.10787275621. Total steps: 368\n",
      "Episode: 4, Episodic Reward: -19302.239250120645. Total steps: 289\n",
      "Episode: 5, Episodic Reward: -32561.322313501794. Total steps: 475\n",
      "Episode: 6, Episodic Reward: -48899.4974335141. Total steps: 479\n",
      "Episode: 7, Episodic Reward: -68432.63095576169. Total steps: 694\n",
      "Episode: 8, Episodic Reward: -39685.255396319706. Total steps: 361\n",
      "Episode: 9, Episodic Reward: -20294.072988608474. Total steps: 280\n",
      "Episode: 10, Episodic Reward: -90934.3930165105. Total steps: 917\n",
      "Episode: 11, Episodic Reward: -25433.17562901804. Total steps: 280\n",
      "Episode: 12, Episodic Reward: -115693.91442757202. Total steps: 942\n",
      "Episode: 13, Episodic Reward: -36894.68901459803. Total steps: 596\n",
      "Episode: 14, Episodic Reward: -19834.43462297728. Total steps: 282\n",
      "Episode: 15, Episodic Reward: -60374.39140679018. Total steps: 963\n",
      "Episode: 16, Episodic Reward: -25085.381374839082. Total steps: 214\n",
      "Episode: 17, Episodic Reward: -22362.763657150557. Total steps: 255\n",
      "Episode: 18, Episodic Reward: -20966.502363208736. Total steps: 350\n",
      "Episode: 19, Episodic Reward: -18505.52351811659. Total steps: 252\n",
      "Episode: 20, Episodic Reward: -37800.033035588924. Total steps: 555\n",
      "Episode: 21, Episodic Reward: -17225.45135908375. Total steps: 197\n",
      "Episode: 22, Episodic Reward: -18923.8918059718. Total steps: 261\n",
      "Episode: 23, Episodic Reward: -28204.7738161986. Total steps: 566\n",
      "Episode: 24, Episodic Reward: -29264.805884348487. Total steps: 499\n",
      "Episode: 25, Episodic Reward: -37013.72132233357. Total steps: 417\n",
      "Episode: 26, Episodic Reward: -43446.229687768966. Total steps: 460\n",
      "Episode: 27, Episodic Reward: -39623.40658213756. Total steps: 485\n",
      "Episode: 28, Episodic Reward: -141150.50123369897. Total steps: 1938\n",
      "Episode: 29, Episodic Reward: -35705.44830967665. Total steps: 374\n",
      "Episode: 30, Episodic Reward: -20562.62218230903. Total steps: 280\n",
      "Episode: 31, Episodic Reward: -36424.13187908316. Total steps: 432\n",
      "Episode: 32, Episodic Reward: -78679.90526855126. Total steps: 582\n",
      "Episode: 33, Episodic Reward: -139087.16045942908. Total steps: 604\n",
      "Episode: 34, Episodic Reward: -75791.67154007846. Total steps: 667\n",
      "Episode: 35, Episodic Reward: -20312.22567695462. Total steps: 453\n",
      "Episode: 36, Episodic Reward: -50694.798251540466. Total steps: 781\n",
      "Episode: 37, Episodic Reward: -82590.96438507421. Total steps: 722\n",
      "Episode: 38, Episodic Reward: -19280.736218946702. Total steps: 327\n",
      "Episode: 39, Episodic Reward: -35435.690423992026. Total steps: 437\n",
      "Episode: 40, Episodic Reward: -99028.65813734336. Total steps: 1229\n",
      "Episode: 41, Episodic Reward: -44496.68555899788. Total steps: 441\n",
      "Episode: 42, Episodic Reward: -50346.25591199987. Total steps: 421\n",
      "Episode: 43, Episodic Reward: -21818.410325807556. Total steps: 433\n",
      "Episode: 44, Episodic Reward: -43342.83491636612. Total steps: 578\n",
      "Episode: 45, Episodic Reward: -28749.285226352844. Total steps: 340\n",
      "Episode: 46, Episodic Reward: -25076.58967470419. Total steps: 416\n",
      "Episode: 47, Episodic Reward: -135006.09865090623. Total steps: 1473\n",
      "Episode: 48, Episodic Reward: -17074.641471808507. Total steps: 261\n",
      "Episode: 49, Episodic Reward: -24633.031754742515. Total steps: 325\n",
      "Episode: 50, Episodic Reward: -34010.12356613901. Total steps: 347\n",
      "Episode: 51, Episodic Reward: -19414.842587649826. Total steps: 271\n",
      "Episode: 52, Episodic Reward: -17388.672820492604. Total steps: 268\n",
      "Episode: 53, Episodic Reward: -52570.858353262614. Total steps: 700\n",
      "Episode: 54, Episodic Reward: -32290.172054118084. Total steps: 557\n",
      "Episode: 55, Episodic Reward: -26710.25834027412. Total steps: 412\n",
      "Episode: 56, Episodic Reward: -20159.84605045742. Total steps: 273\n",
      "Episode: 57, Episodic Reward: -22121.53176630223. Total steps: 348\n",
      "Episode: 58, Episodic Reward: -40570.655165416545. Total steps: 404\n",
      "Episode: 59, Episodic Reward: -32779.18015016938. Total steps: 450\n",
      "Episode: 60, Episodic Reward: -17775.95081510662. Total steps: 260\n",
      "Episode: 61, Episodic Reward: -56578.346076799404. Total steps: 682\n",
      "Episode: 62, Episodic Reward: -38728.049432363914. Total steps: 241\n",
      "Episode: 63, Episodic Reward: -20941.660193552307. Total steps: 418\n",
      "Episode: 64, Episodic Reward: -75796.39705627332. Total steps: 917\n",
      "Episode: 65, Episodic Reward: -68955.10871796642. Total steps: 640\n",
      "Episode: 66, Episodic Reward: -51809.111821984414. Total steps: 655\n",
      "Episode: 67, Episodic Reward: -44949.209951206336. Total steps: 515\n",
      "Episode: 68, Episodic Reward: -41161.34021493616. Total steps: 398\n",
      "Episode: 69, Episodic Reward: -69792.9259498209. Total steps: 644\n",
      "Episode: 70, Episodic Reward: -28664.781766013668. Total steps: 447\n",
      "Episode: 71, Episodic Reward: -22362.828220644053. Total steps: 255\n",
      "Episode: 72, Episodic Reward: -174109.21303838698. Total steps: 1130\n",
      "Episode: 73, Episodic Reward: -17941.537563900718. Total steps: 225\n",
      "Episode: 74, Episodic Reward: -45628.36863941494. Total steps: 381\n",
      "Episode: 75, Episodic Reward: -22863.467185211335. Total steps: 267\n",
      "Episode: 76, Episodic Reward: -42764.17392235969. Total steps: 433\n",
      "Episode: 77, Episodic Reward: -69274.63108200447. Total steps: 707\n",
      "Episode: 78, Episodic Reward: -23506.706352468555. Total steps: 299\n",
      "Episode: 79, Episodic Reward: -37116.67844628361. Total steps: 386\n",
      "Episode: 80, Episodic Reward: -20984.92703313483. Total steps: 273\n",
      "Episode: 81, Episodic Reward: -47394.65582740302. Total steps: 547\n",
      "Episode: 82, Episodic Reward: -23782.625416803767. Total steps: 347\n"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "rewards = train(agent, buffer, 256, 10000, 2.0, 1.0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample!(buffer, 1)\n",
    "# state = get_state(model_env.drone)\n",
    "# println(state)\n",
    "action = get_action(agent, model_env.action, model_env.target_position, get_state(model_env.drone))\n",
    "println(action)\n",
    "# println(model_env.target_position)\n",
    "# println(vcat(state, action, model_env.target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = DroneEnv([1.0, 1.0, 1.0], 2.0)\n",
    "evaluate(agent, eval_env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_position = [rand(-10:10), rand(-10:10), rand(1:10)]\n",
    "# target_position = convert(Array{Float64, 1}, target_position)\n",
    "# println(\"Target position: $(target_position)\")\n",
    "# model_env = DroneEnv(target_position, 1.0)\n",
    "# state_space = vcat(get_normalised_state(model_env.drone), model_env.target_position, model_env.action)\n",
    "# output = agent.actor.model(state_space)\n",
    "model_env = DroneEnv([1.0, 1.0, 1.0], 2.0)\n",
    "state_spaces, motors_rpm = run_model(agent, model_env)\n",
    "println(get_state(model_env.drone), model_env.action)\n",
    "plotStateSpaces(state_spaces)\n",
    "plotMotorRpms(motors_rpm)\n",
    "# println(motors_rpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2223.8812148571014\n",
    "# 547.933466732502\n",
    "# 681.3510209321976\n",
    "# 1113.097071647644\n",
    "\n",
    "state = get_normalised_state(model_env.drone)\n",
    "addParamterNoise!(agent, 0.001)\n",
    "action = get_action(agent, model_env.action, model_env.target_position, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(agent, \"agent_actor_model.bson\", \"agent_critic_model.bson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNoise!(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update the agent weights with noise\n",
    "# old_params = Flux.params(agent.actor.model)\n",
    "# new_params = Flux.params(agent.actor.model)\n",
    "# params_vector = convert(Array{Float64, 1}, Flux.params(agent.actor.model))\n",
    "# for param in new_params\n",
    "# \t# for i in 1:length(new_params)\n",
    "# \ttemp_state = zeros(size(param))\n",
    "# \t# temp_state = zeros(size(new_params[i]))\n",
    "# \tnoise_rand = randn!(temp_state) * 0.1\n",
    "# \ttemp_state += noise_rand\n",
    "# \tparam = temp_state\n",
    "# \t# noise_rand = randn!(ones(size(param))) * 0.1\n",
    "# \t# noise_rand = noise_rand |> gpu\n",
    "# \t# param .+= noise_rand\n",
    "# end\n",
    "\n",
    "println(\"Check if the parameters are changed: \", old_params != new_params)\n",
    "# new_params = Flux.params(agent.actor.model)\n",
    "# println(\"Check if the parameters are changed: \", old_params != new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Check if the parameters are changed: \", old_params != Flux.params(agent.actor.model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_1 = copy(agent_params[1])\n",
    "check_noise = randn!(ones(size(agent_params[1]))) * 0.1\n",
    "check_noise = check_noise |> gpu\n",
    "check_1 .+= check_noise\n",
    "println(\"Check if the parameters are changed: \", check_1 != agent_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNoise!(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = env.drone.position\n",
    "previous_position = env.previous_position\n",
    "velocity = env.drone.velocity\n",
    "orientation = env.drone.orientation\n",
    "angular_velocity = env.drone.angular_velocity\n",
    "target_position = env.target_position\n",
    "\n",
    "# Calculate the distance to the target\n",
    "dist_to_target = norm(position - target_position)\n",
    "\n",
    "# Calculate the distance to the target in the previous step\n",
    "prev_dist_to_target = norm(previous_position - target_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model_state = Flux.state(agent.actor.model);\n",
    "critic_model_state = Flux.state(agent.critic.model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jldsave(\"agent_actor_model.jld2\"; actor_model_state)\n",
    "jldsave(\"agent_critic_model.jld2\"; critic_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"BSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model_state = Flux.state(agent.actor.model);\n",
    "critic_model_state = Flux.state(agent.critic.model);\n",
    "\n",
    "using JLD2\n",
    "jldsave(\"agent_actor_model.jld2\"; actor_model_state)\n",
    "jldsave(\"agent_critic_model.jld2\"; critic_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON: @save, @load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_actor = Actor(state_dim, action_dim, max_action)\n",
    "# temp_actor_model = Flux.cpu(temp_actor.model);\n",
    "@load \"agent_actor_model.bson\" actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(actor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = JLD2.load(\"agent_actor_model.jld2\", \"actor_model_state\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Actor(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model = model.model |> cpu\n",
    "temp_model = model.model |> cpu\n",
    "Flux.loadparams!(temp_model, Flux.params(model_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.loadmodel!(model.model, model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deserialize(\"file.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
