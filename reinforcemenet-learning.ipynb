{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.add(\"ReinforcementLearning\")\n",
    "Pkg.add(\"ReinforcementLearningBase\")\n",
    "Pkg.add(\"ReinforcementLearningCore\")\n",
    "Pkg.add(\"CUDA\")\n",
    "Pkg.add(\"CompilerPluginTools\")\n",
    "Pkg.add(\"PlotlyJS\")\n",
    "Pkg.add(\"DifferentialEquations\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Flux\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "using Plots\n",
    "using ReinforcementLearningBase\n",
    "using ReinforcementLearningCore\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Flux\n",
    "using Distributions\n",
    "using Flux: params, gradient\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA GeForce RTX 3080"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0225"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass = 1.0\n",
    "force_per_rpm = 4e-10\n",
    "distance_to_propellers = 0.15\n",
    "moment_of_inertia = mass * distance_to_propellers^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ContinuousDroneActionSpace\n",
    "    action_range::Tuple{CuArray{Float64}, CuArray{Float64}} # define action_range within the space struct\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ContinuousDroneStateSpace\n",
    "\tposition_low::CuArray{Float64,1}\n",
    "\tposition_high::CuArray{Float64,1}\n",
    "\tvelocity_low::CuArray{Float64,1}\n",
    "\tvelocity_high::CuArray{Float64,1}\n",
    "\torientation_low::CuArray{Float64,1}\n",
    "\torientation_high::CuArray{Float64,1}\n",
    "\tangular_velocity_low::CuArray{Float64,1}\n",
    "\tangular_velocity_high::CuArray{Float64,1}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: rand\n",
    "\n",
    "function rand(rng::AbstractRNG, space::ContinuousDroneActionSpace, num::Int)\n",
    "    low, high = space.action_range\n",
    "    return [rand(rng, low[i]:high[i]) for i in 1:length(low), _ in 1:num]\n",
    "end\n",
    "\n",
    "import Random\n",
    "\n",
    "function Random.Sampler(rng::Random.AbstractRNG, space::ContinuousDroneActionSpace, ::Random.Repetition)\n",
    "    return space\n",
    "end\n",
    "\n",
    "function Random.rand(rng::Random.AbstractRNG, sampler::ContinuousDroneActionSpace)\n",
    "    low, high = sampler.action_range\n",
    "    return [rand(rng, low[i]:high[i]) for i in 1:length(low)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Drone\n",
    "\tposition::CuArray{Float64,1}\n",
    "\tvelocity::CuArray{Float64,1}\n",
    "\torientation::CuArray{Float64,1} # roll, pitch, yaw in radians\n",
    "\tangular_velocity::CuArray{Float64,1}\n",
    "\n",
    "\tmass::Float64\n",
    "\tforce_per_rpm::Float64\n",
    "\tdistance_to_propellers::Float64\n",
    "\tmoment_of_inertia::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DroneEnv"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct DroneEnv <: AbstractEnv\n",
    "    drone::Drone\n",
    "    target_position::CuArray{Float64,1}\n",
    "    previous_position::CuArray{Float64,1}\n",
    "    action::CuArray{Float64,1}\n",
    "    reward::Float64\n",
    "    done::Bool\n",
    "\n",
    "    velocity_threshold::Float64\n",
    "    alititude_threshold::Float64\n",
    "    angular_velocity_threshold::Float64\n",
    "    roll_threshold::Float64\n",
    "    pitch_threshold::Float64\n",
    "\n",
    "    time::Float64\n",
    "    max_time::Float64\n",
    "end\n",
    "\n",
    "function DroneEnv()\n",
    "    drone = Drone(zeros(3), zeros(3), zeros(3), zeros(3), mass, force_per_rpm, distance_to_propellers, moment_of_inertia)\n",
    "    action = zeros(4)\n",
    "    target_position = [10.0, 10.0, 10.0]\n",
    "    previous_position = zeros(3)\n",
    "    DroneEnv(drone, target_position, previous_position, action, 0.0, false, 10, 10, 5, pi/4, pi/4, 0, 100)\n",
    "end\n",
    "\n",
    "function DroneEnv(target_position::CuArray{Float64,1})\n",
    "    drone = Drone(zeros(3), zeros(3), zeros(3), zeros(3), mass, force_per_rpm, distance_to_propellers, moment_of_inertia)\n",
    "    action = zeros(4)\n",
    "    previous_position = zeros(3)\n",
    "    DroneEnv(drone, target_position, previous_position, action, 0.0, false, 10, 10, 5, pi/4, pi/4, 0, 100)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Action Space for quadcopter rpm of each motor\n",
    "RLBase.action_space(env::DroneEnv) = ContinuousDroneActionSpace((zeros(4), ones(4)*20000))\n",
    "RLBase.state(env::DroneEnv) = [env.drone.position; env.drone.velocity; env.drone.orientation; env.drone.angular_velocity; env.drone.mass; env.drone.force_per_rpm; env.drone.distance_to_propellers; env.drone.moment_of_inertia]\n",
    "# RLBase.is_terminated(env::DroneEnv) = env.done\n",
    "RLBase.state_space(env::DroneEnv) = ContinuousDroneStateSpace(\n",
    "\t[-100.0, -10.0, 0.0],\n",
    "\t[100.0, 100.0, 100.0], \n",
    "\t[-10.0, -10.0, -10.0],\n",
    "\t[10.0, 10.0, 10.0],\n",
    "\t[-pi, -pi, -pi],\n",
    "\t[pi/1, pi/1, pi/1],\n",
    "\t[-pi, -pi, -pi],\n",
    "\t[pi/1, pi/1, pi/1],\n",
    ")\n",
    "# RLBase.action_space(env::DroneEnv, action::CuArray{Float64,1}) = ContinuousDroneActionSpace(zeros(4), ones(4)*2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::DroneEnv)\n",
    "    drone = Drone(zeros(3), zeros(3), zeros(3), zeros(3), mass, force_per_rpm, distance_to_propellers, moment_of_inertia)\n",
    "    env.previous_position = zeros(3)\n",
    "    env.done = false\n",
    "    env.reward = 0.0\n",
    "    env.time = 0.0\n",
    "    env.drone = drone\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drone_dynamics! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function drone_dynamics!(du, u, p, t)\n",
    "    gravity = 9.81 # gravitational constant, m/s^2\n",
    "    rpms = p(t) # propeller speeds in RPM at time t\n",
    "    \n",
    "    ### Get the mass, force_per_rpm, distance_to_propellers, and moment_of_inertia from the drone\n",
    "    mass = u[13]\n",
    "    force_per_rpm = u[14]\n",
    "    distance_to_propellers = u[15]\n",
    "    moment_of_inertia = u[16]\n",
    "\n",
    "\n",
    "    ### force generated by each propeller is squared to account for the fact that the force is proportional to the square of the propeller speed\n",
    "    forces = [force_per_rpm*(rpms[1]^2), force_per_rpm*(rpms[2]^2), force_per_rpm*(rpms[3]^2), force_per_rpm*(rpms[4]^2)] # force generated by each propeller\n",
    "    net_force = sum(forces) - mass*gravity # net force (upward force - weight)\n",
    "    net_torque_pitch = (forces[1] + forces[4] - forces[2] - forces[3])*distance_to_propellers # net torque around x-axis\n",
    "    net_torque_roll = (forces[1] + forces[2] - forces[3] - forces[4])*distance_to_propellers # net torque around y-axis\n",
    "    net_torque_yaw = (forces[1] + forces[3] - forces[2] - forces[4])*distance_to_propellers # net torque around z-axis\n",
    "\n",
    "\n",
    "    du[1:3] = u[4:6] # position updates with velocity\n",
    "\n",
    "    # Update the x and z velocity and position based on pitch roll and yaw torques\n",
    "    # du[4] = net_t\n",
    "\n",
    "    # du[1] = du[1]*cos(u[6]) - du[2]*sin(u[6])\n",
    "    # du[2] = du[1]*sin(u[6]) + du[2]*cos(u[6])\n",
    "\n",
    "    du[4:6] = [u[7]*cos(u[6]) - u[8]*sin(u[6]), u[7]*sin(u[6]) + u[8]*cos(u[6]), net_force/mass] # velocity updates with acceleration\n",
    "\n",
    "    # Check if drone is on the ground\n",
    "    if du[3] < 0.0\n",
    "        du[3] = 0.0\n",
    "        du[6] = 0.0\n",
    "    end\n",
    "\n",
    "    # Check if drone is upside down\n",
    "    if u[3] > pi/2 || u[3] < -pi/2\n",
    "        du[4:6] = [0.0, 0.0, 0.0]\n",
    "    end\n",
    "\n",
    "    du[7:9] = u[10:12] # orientation updates with angular velocity\n",
    "    du[10:12] = [net_torque_roll/moment_of_inertia, net_torque_pitch/moment_of_inertia, net_torque_yaw/moment_of_inertia] # angular velocity updates with angular acceleration\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardFunction (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function RewardFunction(env::DroneEnv)\n",
    "# \ttot_reward = 0.0\n",
    "# \tprev_dist_to_target = norm(env.previous_position - env.target_position)\n",
    "# \tdist_to_target = norm(env.drone.position - env.target_position)\n",
    "# \tyaw = env.drone.orientation[3]\n",
    "# \troll = env.drone.orientation[1]\n",
    "# \tpitch = env.drone.orientation[2]\n",
    "\n",
    "# \ttot_reward -= dist_to_target\n",
    "\n",
    "# \tif roll > pi/2 || roll < -pi/2 || pitch > pi/4 || pitch < -pi/4\n",
    "# \t\ttot_reward -= 1000\n",
    "# \tend\n",
    "\n",
    "# \tfacing_dir = [cos(yaw), sin(yaw)]\n",
    "# \tdrone_dir = [cos(roll)*cos(pitch), sin(roll)*cos(pitch)]\n",
    "# \tif dot(facing_dir, drone_dir) < 0.0\n",
    "# \t\ttot_reward -= 1000\n",
    "# \tend\n",
    "\n",
    "# \tif dot(env.drone.velocity, env.target_position - env.drone.position) < 0.0\n",
    "# \t\ttot_reward -= 1000\n",
    "# \tend\n",
    "\t\n",
    "# \tif env.drone.angular_velocity[1] != 0.0 || env.drone.angular_velocity[2] != 0.0\n",
    "# \t\ttot_reward -= abs(sum(env.drone.angular_velocity[1:2]))\n",
    "# \tend\n",
    "\n",
    "# \tif env.drone.angular_velocity[3] != 0.0\n",
    "# \t\ttot_reward -= abs(env.drone.angular_velocity[3])\n",
    "# \tend\n",
    "\n",
    "# \tif dist_to_target < prev_dist_to_target\n",
    "# \t\ttot_reward += 100\n",
    "# \tend\n",
    "# \tif dist_to_target < 0.1\n",
    "# \t\ttot_reward += 1000\n",
    "# \tend\n",
    "\n",
    "# \treturn tot_reward\n",
    "# end\n",
    "\n",
    "function RewardFunction(env::DroneEnv)\n",
    "    # Compute the distance to the target\n",
    "    distance_to_target = norm(env.drone.position - env.target_position)\n",
    "\n",
    "    # Compute the energy cost\n",
    "    energy_cost = sum(env.action)\n",
    "\n",
    "    # Reward is higher for smaller distances to target and lower energy costs\n",
    "    reward = -distance_to_target - 0.01 * energy_cost\n",
    "\n",
    "    return reward\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.is_terminated(env::DroneEnv)\n",
    "\t### Check if the drone orientation is too far from vertical\n",
    "\troll = env.drone.orientation[1]\n",
    "\tpitch = env.drone.orientation[2]\n",
    "\tif abs(roll) > env.roll_threshold || abs(pitch) > env.pitch_threshold\n",
    "\t\t# println(\"roll or pitch too large\")\n",
    "\t\tenv.done = true\n",
    "\tend\n",
    "\n",
    "\tif norm(env.drone.velocity) > env.velocity_threshold\n",
    "\t\t# println(\"velocity too large\")\n",
    "\t\tenv.done = true\n",
    "    end\n",
    "\n",
    "\tif norm(env.drone.angular_velocity) > env.angular_velocity_threshold\n",
    "\t\t# println(\"angular velocity too large\")\n",
    "\t\tenv.done = true\n",
    "\tend\n",
    "\n",
    "\tif env.drone.position[3] < 0.0\n",
    "\t\t# println(\"drone crashed\")\n",
    "\t\tenv.done = true\n",
    "\tend\n",
    "\n",
    "\tif norm([env.drone.position[1] - env.target_position[1]; env.drone.position[2] - env.target_position[2]; env.drone.position[3] - env.target_position[3]]) < 0.1\n",
    "\t\t# println(\"drone reached target\")\t\t\n",
    "\t\tenv.done = true\n",
    "\tend\n",
    "\n",
    "\tif env.time > env.max_time\n",
    "\t\t# println(\"time limit reached\")\n",
    "\t\tenv.done = true\n",
    "\tend\n",
    "\n",
    "\treturn env.done\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function step!(env::DroneEnv, action::CuArray{Float64,1, CUDA.Mem.DeviceBuffer})\n",
    "\t# action is an array of 4 numbers\n",
    "\t# action[1] is the rpm of the front left propeller\n",
    "\t# action[2] is the rpm of the front right propeller\n",
    "\t# action[3] is the rpm of the back left propeller\n",
    "\t# action[4] is the rpm of the back right propeller\n",
    "\ttspan = (0.0, 0.1) # time span for the differential equation solver\n",
    "\tp = (t) -> action # propeller speeds as a function of time\n",
    "\tenv.previous_position = env.drone.position # save the previous position\n",
    "\tenv.action = action # save the action\n",
    "\tu0 = [env.drone.position; env.drone.velocity; env.drone.orientation; env.drone.angular_velocity; env.drone.mass; env.drone.force_per_rpm; env.drone.distance_to_propellers; env.drone.moment_of_inertia] # initial state\n",
    "\tprob = ODEProblem(drone_dynamics!, u0, tspan, p) # define the ODE problem\n",
    "\tsol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8) # solve the ODE problem\n",
    "\tenv.drone.position = sol.u[end][1:3] # update the drone position\n",
    "\tenv.drone.velocity = sol.u[end][4:6] # update the drone velocity\n",
    "\tenv.drone.orientation = sol.u[end][7:9] # update the drone orientation\n",
    "\tenv.drone.angular_velocity = sol.u[end][10:12] # update the drone angular velocity\n",
    "\tenv.reward = RewardFunction(env) # update the reward\n",
    "\tenv.time += 0.1 # update the time\n",
    "\t\n",
    "\t\n",
    "\treturn env.reward, env.done\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function get_state(drone::Drone)\n",
    "#     return [drone.position..., drone.velocity..., drone.orientation..., drone.angular_velocity...]\n",
    "# end\n",
    "\n",
    "# function apply_changes!(drone::Drone, du)\n",
    "#     # This function should update the drone's state based on the changes calculated by drone_dynamics!.\n",
    "#     # For example:\n",
    "#     drone.position .+= du[1:3]\n",
    "#     drone.velocity .+= du[4:6]\n",
    "#     drone.orientation .+= du[7:9]\n",
    "#     drone.angular_velocity .+= du[10:12]\n",
    "# end\n",
    "\n",
    "# function ReinforcementLearningBase.step!(env::DroneEnv, action)\n",
    "#     # Apply the action (change in RPMs or rotor forces), then update drone's state.\n",
    "#     # Use the drone_dynamics! function here.\n",
    "#     du = zeros(12) #assuming the size of du matches with the state size\n",
    "#     drone_dynamics!(du, get_state(env.drone), (t) -> action, 0.0) # assuming that drone_dynamics! is defined in a way that is compatible with this use\n",
    "\n",
    "#     # Update drone state with the changes\n",
    "#     apply_changes!(env.drone, du)\n",
    "\n",
    "#     # Also update the reward and done status.\n",
    "#     env.reward = reward(env, action)\n",
    "#     env.done = is_done(env)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DroneEnv()\n",
    "\n",
    "# total_reward = 0.0\n",
    "# num_steps = 0\n",
    "\n",
    "# trajectory = []\n",
    "\n",
    "# # Start the episode\n",
    "# while !RLBase.is_terminated(env)\n",
    "#     # Select a random action\n",
    "#     action = rand(RLBase.action_space(env))\n",
    "\n",
    "#     # Apply the action to the environment\n",
    "#     reward, done = env(action)\n",
    "# \t# println(\"Reward: \", reward)\n",
    "\n",
    "#     # Update the total reward and number of steps\n",
    "#     total_reward += reward\n",
    "#     num_steps += 1\n",
    "\n",
    "#     # Save the drone's position\n",
    "#     push!(trajectory, env.drone.position)\n",
    "\n",
    "#     if done\n",
    "#         break\n",
    "#     end\n",
    "# end\n",
    "# println(\"Episode finished after \", num_steps, \" steps. Total reward: \", total_reward)\n",
    "# println(\"Final position: \", env.drone.position)\n",
    "\n",
    "# ### 3d Plot the trajectory\n",
    "# x = [trajectory[i][1] for i in 1:length(trajectory)]\n",
    "# y = [trajectory[i][2] for i in 1:length(trajectory)]\n",
    "# z = [trajectory[i][3] for i in 1:length(trajectory)]\n",
    "\n",
    "# # plotlyjs()\n",
    "# plot(x, y, z, label=\"trajectory\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\", title=\"Drone Trajectory\", legend=:bottomright)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Start the reinforcement learning\n",
    "# ### Implement a Deep learning model for Q-learning algorithm to learn the optimal policy\n",
    "# ### The state space is the drone's position and velocity\n",
    "# ### The action space is the change in RPMs of the propellers\n",
    "# ### The reward function is the distance to the target\n",
    "# ### The termination condition is when the drone reaches the target or crashes\n",
    "\n",
    "# ### Define the Q-network\n",
    "# struct QNetwork\n",
    "# \tenv::DroneEnv\n",
    "# end\n",
    "\n",
    "# ### Define the DeepQLearningPolicy\n",
    "# struct DeepQLearningPolicy\n",
    "# \tenv::DroneEnv\n",
    "# \tQ::QNetwork\n",
    "# \tepsilon::Float64\n",
    "# \tgamma::Float64\n",
    "# \talpha::Float64\n",
    "# \tepsilon_decay::Float64\n",
    "# \tgamma_decay::Float64\n",
    "# \talpha_decay::Float64\n",
    "# \tepsilon_min::Float64\n",
    "# \tgamma_min::Float64\n",
    "# \talpha_min::Float64\n",
    "# end\n",
    "\n",
    "# policy = DeepQLearningPolicy(env, QNetwork(env), 0.1, 0.99, 0.1, 0.1, 0.1, 0.1, 0.1, 0.95, 0.1)\n",
    "\n",
    "# ### Define the Q-learning algorithm\n",
    "# function QLearning(policy::DeepQLearningPolicy, num_episodes::Int64)\n",
    "# \t# Initialize the Q-network\n",
    "# \tQ = policy.Q\n",
    "# \t# Initialize the total reward\n",
    "# \ttotal_reward = 0.0\n",
    "# \t# Initialize the number of steps\n",
    "# \tnum_steps = 0\n",
    "# \t# Initialize the trajectory\n",
    "# \ttrajectory = []\n",
    "# \t# Initialize the episode\n",
    "# \tfor i in 1:num_episodes\n",
    "# \t\t# Initialize the environment\n",
    "# \t\tenv = policy.env\n",
    "# \t\t# Initialize the episode\n",
    "# \t\twhile !RLBase.is_terminated(env)\n",
    "# \t\t\t# Select an action\n",
    "# \t\t\taction = rand(RLBase.action_space(env))\n",
    "# \t\t\t# Apply the action to the environment\n",
    "# \t\t\treward, done = env(action)\n",
    "# \t\t\t# Update the total reward and number of steps\n",
    "# \t\t\ttotal_reward += reward\n",
    "# \t\t\tnum_steps += 1\n",
    "# \t\t\t# Save the drone's position\n",
    "# \t\t\tpush!(trajectory, env.drone.position)\n",
    "# \t\t\t# Update the Q-network\n",
    "# \t\t\tQ = update(Q, env, action, reward, done)\n",
    "# \t\t\t# Update the policy\n",
    "# \t\t\tpolicy = update(policy, Q)\n",
    "# \t\t\tif done\n",
    "# \t\t\t\tbreak\n",
    "# \t\t\tend\n",
    "# \t\tend\n",
    "# \tend\n",
    "# \treturn policy, Q, total_reward, num_steps, trajectory\n",
    "# end\n",
    "\n",
    "# ### Define the update function for the Q-network\n",
    "# function update(Q::QNetwork, env::DroneEnv, action, reward, done)\n",
    "# \t# Update the Q network based on the reward and done status\n",
    "\t\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOPolicy"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Implement the PPO algorithm\n",
    "### Define the PPO policy\n",
    "mutable struct PPOPolicy\n",
    "\tenv::DroneEnv\n",
    "\tpolicy::Chain\n",
    "\tbatch::Array{Tuple{CuArray{Float64,1}, CuArray{Float64,1}, Float64, CuArray{Float64,1}, Bool}, 1}\n",
    "\tepsilon::Float64\n",
    "\tgamma::Float64\n",
    "\talpha::Float64\n",
    "\tepsilon_decay::Float64\n",
    "\tgamma_decay::Float64\n",
    "\talpha_decay::Float64\n",
    "\tepsilon_min::Float64\n",
    "\tgamma_min::Float64\n",
    "\talpha_min::Float64\n",
    "end\n",
    "\n",
    "function PPOPolicy(env::DroneEnv, policy::Chain, epsilon::Float64, gamma::Float64, alpha::Float64, epsilon_decay::Float64, gamma_decay::Float64, alpha_decay::Float64, epsilon_min::Float64, gamma_min::Float64, alpha_min::Float64)\n",
    "\tbatch = [] # Empty batch to start\n",
    "\treturn PPOPolicy(env, policy, batch, epsilon, gamma, alpha, epsilon_decay, gamma_decay, alpha_decay, epsilon_min, gamma_min, alpha_min)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gaussian_policy(network::Chain, state::CuArray{Float64,1})\n",
    "\tmu, log_sigma = network(state)  # Assume the network outputs means and log standard deviations\n",
    "    sigma = exp.(log_sigma)\n",
    "    return MvNormal(mu, sigma)  # Return a multivariate normal distribution\n",
    "end\n",
    "\n",
    "function prob(policy::PPOPolicy, state::CuArray{Float64,1}, action::CuArray{Float64,1})\n",
    "\tdistribution = gaussian_policy(policy.policy, state)\n",
    "    return pdf(distribution, action)  # Probability density function for the given action\n",
    "end\n",
    "\n",
    "function get_action(policy::PPOPolicy, state::CuArray{Float64,1})\n",
    "\tmu, log_sigma = policy.policy(state)  # Assume the network outputs means and log standard deviations\n",
    "    sigma = exp.(log_sigma)\n",
    "\n",
    "\tmu = Array(mu)\n",
    "\tsigma = Array(sigma)\n",
    "\n",
    "\tdistribution = MvNormal(mu, sigma)\n",
    "\taction = rand(distribution)\n",
    "\taction = action |> gpu\n",
    "\treturn action\n",
    "end\n",
    "\n",
    "### Define the update function for the PPO policy\n",
    "function update(policy::PPOPolicy, optimizer)\n",
    "\t# loss = -log(prob(policy, state, action)) * reward  # Minus sign because we're maximizing reward\n",
    "    # grads = gradient(() -> loss, params(policy.policy))\n",
    "    # Flux.Optimise.update!(optimizer, params(policy.policy), grads)\n",
    "\tfor (state, action, reward, new_state, done) in policy.batch\n",
    "\t\tstate = state |> gpu\n",
    "\t\taction = action |> gpu\n",
    "\n",
    "\t\tstate = convert(CuArray{Float64,1}, state)\n",
    "\t\taction = convert(CuArray{Float64,1}, action)\n",
    "\n",
    "\t\taction_new = get_action(policy, state)\n",
    "\t\taction_new = action_new |> gpu\n",
    "\t\taction_new = convert(CuArray{Float64,1}, action_new)\n",
    "\n",
    "\t\tvalue = reward + policy.gamma * (done ? 0.0 : prob(policy, new_state, action_new))\n",
    "\t\tadvantage = value - prob(policy, state, action)\n",
    "\t\tratio = prob(policy, state, action) / prob(policy, state, action)\n",
    "\t\tloss = -min(ratio * advantage, clamp(ratio, 1 - policy.epsilon, 1 + policy.epsilon) * advantage)\n",
    "\t\tgrads = gradient(() -> loss, params(policy.policy))\n",
    "\t\tFlux.Optimise.update!(optimizer, params(policy.policy), grads)\n",
    "\tend\n",
    "\tpolicy.batch = []  # Empty the batch\n",
    "end\n",
    "\n",
    "\n",
    "### Define the PPO algorithm\n",
    "function PPO(policy::PPOPolicy, num_episodes::Int64, batch_size::Int64)\n",
    "\t# Initialize the total reward\n",
    "\ttotal_reward = 0.0\n",
    "\t# Initialize the number of steps\n",
    "\tnum_steps = 0\n",
    "\t# Initialize the trajectory\n",
    "\ttrajectories = []\n",
    "\toptimizer = ADAM(0.01)\n",
    "\n",
    "\t# Initialize the episode\n",
    "\tfor i in 1:num_episodes\n",
    "\t\ttrajectory = []\n",
    "\t\t# Initialize the environment\n",
    "\t\tenv = policy.env\n",
    "\t\t# Reset the environment\n",
    "\t\tRLBase.reset!(env)\n",
    "\t\t# Initialize the episode\n",
    "\t\twhile !RLBase.is_terminated(env)\n",
    "\t\t\tstate = RLBase.state(env)\n",
    "\t\t\tstate = state |> gpu\n",
    "\t\t\tstate = convert(CuArray{Float64,1}, state)\n",
    "\n",
    "\t\t\taction = get_action(policy, state)\n",
    "\t\t\taction = action |> gpu\n",
    "\t\t\taction = convert(CuArray{Float64,1}, action)\n",
    "\n",
    "\n",
    "\t\n",
    "\t\t\t# Apply the action to the environment\n",
    "\t\t\treward, done = step!(env, action)\n",
    "\t\t\tpush!(policy.batch, (state, action, reward, state, done))\n",
    "\t\t\t# Update the total reward and number of steps\n",
    "\t\t\ttotal_reward += reward\n",
    "\t\t\tnum_steps += 1\n",
    "\t\t\t# Save the drone's position\n",
    "\t\t\tpush!(trajectory, env.drone.position)\n",
    "\t\t\t# Update the policy\n",
    "\t\t\tif length(policy.batch) >= batch_size\n",
    "\t\t\t\tupdate(policy, optimizer)\n",
    "\t\t\tend\n",
    "\t\t\tif done\n",
    "\t\t\t\tbreak\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\t\tpush!(trajectories, trajectory)\n",
    "\t\t# Decay the exploration\n",
    "\t\tpolicy.epsilon = max(policy.epsilon * policy.epsilon_decay, policy.epsilon_min)\n",
    "\tend\n",
    "\treturn policy, total_reward, num_steps, trajectories\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(16 => 64, relu),                \u001b[90m# 1_088 parameters\u001b[39m\n",
       "  Dense(64 => 64, relu),                \u001b[90m# 4_160 parameters\u001b[39m\n",
       "  Dense(64 => 8),                       \u001b[90m# 520 parameters\u001b[39m\n",
       "  var\"#17#18\"(),\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m5_768 parameters, 856 bytes."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pos = CuArray([1.0, 1.0, 1.0])\n",
    "env = DroneEnv(target_pos)\n",
    "### Create the PPO policy# Define the policy network\n",
    "policy_chain = Chain(\n",
    "    Dense(16, 64, relu),\n",
    "    Dense(64, 64, relu),\n",
    "    Dense(64, 8),  # Assume your state is 4-dimensional, and you output mean and log standard deviation for 4-dimensional action\n",
    "    x -> (x[1:4], x[5:8])  # Split the output into two parts,\n",
    "    # the first part is the mean, the second part is the log standard deviation\n",
    ")\n",
    "policy_chain = policy_chain |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PPOPolicy(env, policy_chain, 0.1, 0.99, 0.1, 0.1, 0.1, 0.1, 0.1, 0.95, 0.1)\n",
    "### Run the PPO algorithm\n",
    "policy, total_reward, num_steps, trajectory = PPO(policy, 1000, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 5-element Vector{Any} at index [6]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 5-element Vector{Any} at index [6]",
      "",
      "Stacktrace:",
      " [1] getindex(A::Vector{Any}, i1::Int64)",
      "   @ Base .\\array.jl:805",
      " [2] top-level scope",
      "   @ In[22]:2"
     ]
    }
   ],
   "source": [
    "for j in 1:1000\n",
    "\ttraj = trajectory[j]\n",
    "\tx = [traj[i][1] for i in 1:length(traj)]\n",
    "\ty = [traj[i][2] for i in 1:length(traj)]\n",
    "\tz = [traj[i][3] for i in 1:length(traj)]\n",
    "\t### Plot the trajectory\n",
    "\tplot(x, y, z, label=\"trajectory\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\", title=\"Drone Trajectory $j\", legend=:bottomright)\n",
    "\t### Save the plot\n",
    "\tsavefig(\"trajectories/trajectory-$j.png\")\n",
    "# plot!(x, y, z)\n",
    "### Plot the drone\n",
    "# scatter!([x[end]], [y[end]], [z[end]], label=\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "89f66e31d3dc4ae08656c8051cd62a33",
   "lastKernelId": "379da302-c674-4dad-b83f-33350176c198"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
